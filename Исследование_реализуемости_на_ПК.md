# Исследование: реализуемость системы «Искусственный разум» на домашнем ПК

**Дата:** 12 февраля 2025  
**Основа:** анализ дизайн-документа «Design_Document_AI_Mind.docx»

---

## 1. Краткое резюме системы из документа

Проект описывает **эмоционально-адаптивную AI-систему** (не «настоящий разум», а имитация эмоционального поведения) из пяти модулей:

| Модуль | Назначение | Ключевые технологии |
|--------|------------|----------------------|
| **Физиологический движок** | Вектор состояния (Energy, SocialDrive, Dopamine, Cortisol, Oxytocin), обновление раз в тик | Собственный код, минимум ресурсов |
| **Сенсорный движок** | Ввод: текст, голос (ASR), видео (VLM + распознавание эмоций) | VLM (CLIP), DeepFace, Whisper |
| **Когнитивное ядро** | Рассуждение, ответы с учётом состояния | LLM (локально Ollama/LM Studio или API) |
| **Мнемоническое ядро** | Краткосрочная + долгосрочная память, Core Identity | ChromaDB / Pinecone, векторный поиск |
| **Эффекторный движок** | Текст, голос (TTS), аватар (Live2D) | StyleTTS2 / ElevenLabs, Live2D |

Документ сам указывает: реалистичный MVP — **6–8 месяцев**, полная система — **12–18 месяцев**; при использовании облачных API стоимость **$1 200–4 000/мес**.

---

## 2. Требования по компонентам (для запуска на своём ПК)

### 2.1. LLM (когнитивное ядро)

| Параметр | Минимум (только инференс) | Комфортно | С дообучением (QLoRA) |
|----------|---------------------------|-----------|-------------------------|
| **VRAM** | 6–8 GB (7B, Q4/KM или IQ3) | 12–16 GB (7B–8B, Q5/Q8) | 10–12 GB для 7B (QLoRA), 16 GB+ для 8B |
| **RAM** | 16 GB | 32 GB | 32 GB |
| **Примечания** | Ollama/LM Studio, GGUF, квантование Q4_K_M | Меньше задержка, можно 8B | RTX 4060 8GB — реально 7B QLoRA по исследованиям |

**Вывод:** на **одной** видеокарте с **8 GB VRAM** можно запускать **7B в Q4** и делать QLoRA для 7B с осторожной настройкой. Для 8B инференса комфортно нужно **12 GB+**.

### 2.2. VLM и распознавание эмоций (сенсоры)

| Компонент | VRAM | RAM | Примечания |
|-----------|------|-----|------------|
| **CLIP** (семантика изображений) | ~1–2 GB | 4 GB | Относительно лёгкий |
| **DeepFace** (эмоции по лицу) | ~1–2 GB | 2–4 GB | 85–92% точности по документу |
| **Камера** | — | — | Обычная веб-камера |

**Вывод:** VLM-часть сенсоров на ПК **реализуема** даже при 8 GB VRAM, если не гонять одновременно с тяжёлым LLM в один момент.

### 2.3. ASR (Whisper)

| Вариант | VRAM | RAM | Скорость |
|---------|------|-----|----------|
| **tiny/base** | 1–2 GB | 4 GB | Быстро на CPU |
| **small/medium** | 2–4 GB | 8 GB | Комфортно на GPU |
| **large-v3** | ~4 GB (инференс) | 16 GB | Желательно GPU, ~3 GB модель |

**Вывод:** на ПК реально **Whisper small/medium** локально; **large-v3** — при 8+ GB VRAM с учётом того, что LLM тоже будет использовать видеокарту.

### 2.4. TTS (StyleTTS2)

- Данных по точным требованиям мало; по отзывам — **работает на CPU и на скромном GPU**.
- Оценка: **2–4 GB VRAM** или работа на CPU (медленнее, но возможно).

### 2.5. Память (ChromaDB) и остальное

- **ChromaDB** — локальная БД, **сотни МБ RAM**, диск по размеру данных.
- **Физиологический движок** — простые формулы, **ресурсы ничтожны**.
- **Live2D** — рендеринг аватара, обычный ПК тянет.

---

## 3. Сводка: можно ли всё собрать на «моём ПК»

Зависит от конфигурации. Ниже — три условных сценария.

### Сценарий A: Слабый ПК (GPU 6–8 GB VRAM, 16 GB RAM)

| Компонент | Реализуемо локально? | Как |
|-----------|----------------------|-----|
| Физиологический движок | ✅ Да | Свой код |
| Память (ChromaDB, Core Identity) | ✅ Да | ChromaDB, файлы |
| LLM | ⚠️ Ограниченно | 7B в Q4_K_M (Ollama/GGUF), одна модель в момент времени |
| VLM (CLIP, DeepFace) | ⚠️ По очереди | Не одновременно с инференсом LLM или с маленьким буфером |
| Whisper | ✅ Да | tiny/small на CPU или GPU по очереди |
| StyleTTS2 | ✅ Да | CPU или GPU при освобождении |
| QLoRA (ночное дообучение) | ⚠️ Только 7B | При 8 GB — по данным исследований возможно с paged optimizers |

**Итог:** **Да, создать систему на таком ПК реально**, но:
- LLM лучше держать в 7B, квантованная (Q4/IQ3);
- Нельзя «все модели сразу в VRAM» — нужна очередь или переключение контекста (сначала сенсоры, потом LLM, потом TTS);
- Задержка (латентность) будет выше, документ как раз указывает 2–6 сек для пайплайна — на слабом ПК может быть 5–15 сек.

### Сценарий B: Средний ПК (GPU 12–16 GB VRAM, 32 GB RAM)

| Компонент | Реализуемо локально? |
|-----------|----------------------|
| Всё из документа | ✅ Да |
| LLM 7B–8B (Q5/Q8) | ✅ Да |
| Одновременная работа LLM + VLM + Whisper (по очереди в пайплайне) | ✅ Да, с разумной очередью |
| QLoRA 7B–8B | ✅ Да |

**Итог:** Систему по документу **реализовать на таком ПК реально** без облачных API, с приемлемой скоростью.

### Сценарий C: Без нормальной видеокарты (только CPU или iGPU)

- **LLM:** только очень маленькие модели (1–3B) или **обязательно облачный API** (OpenAI/Claude и т.д.).
- Whisper, CLIP, DeepFace — на CPU будут **медленными**, но запускаемы.
- **Вывод:** «Полностью на моём ПК» без GPU — **нереалистично** для описанной архитектуры; реалистично только **гибрид**: облачный LLM + остальное локально.

---

## 4. Рекомендации под формулировку «на моём ПК»

1. **Узнать конфигурацию ПК:** GPU (модель и объём VRAM), объём RAM, наличие SSD.
2. **Если VRAM 8 GB:**  
   - считать целевым **MVP с 7B Q4** и поочерёдной загрузкой моделей (или одной LLM в памяти, остальное по запросу);  
   - ночной цикл QLoRA — только для 7B с осторожной настройкой.
3. **Если VRAM 12 GB и выше:**  
   - цели из документа достижимы локально, включая 8B и дообучение.
4. **Если GPU нет или слабая:**  
   - либо вложиться в GPU (например, 12 GB), либо использовать **облачный LLM** (API), остальное оставить локальным — это по-прежнему «система на моём ПК» с частичным облаком.

---

## 5. Ответ на вопрос «реально ли создать такую систему на моём ПК»

**Да, реалистично**, при двух условиях:

1. **Железо:** видеокарта с **минимум 8 GB VRAM** (лучше 12–16 GB) и **16 GB RAM** (лучше 32 GB). Тогда все компоненты из документа можно запускать локально, с ограничениями по размеру модели и латентности при 8 GB.
2. **Оценка сроков и объёма:** как в документе — **6–8 месяцев до MVP**, 12–18 месяцев до полноценной системы; это про трудозатраты и итерации, а не про «нельзя на ПК».

**Нереалистично** ожидать на одном домашнем ПК:
- качества и скорости как у облачных GPT-4/Claude без использования их API;
- одновременной загрузки в VRAM всех моделей (LLM + VLM + Whisper + TTS) при 8 GB — нужна очередность или более мощная карта.

Итог: документ технически корректен; система на ПК **реализуема**, масштаб и комфорт зависят от конфигурации вашего ПК (в первую очередь VRAM и RAM).
