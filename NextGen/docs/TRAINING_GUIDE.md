# SYNAPSE - Философия и Практика Непрерывного Обучения

## Почему НЕ эпохи?

**Мозг не обучается эпохами.**

Эпохи - это искусственное понятие из классического ML:
- "Прогоним данные 100 раз"
- "Каждый пример_seen_ 100 раз"

**Мозг работает иначе:**
- Непрерывный поток опыта
- Каждый момент уникален
- Сон для консолидации
- Рост новых нейронов (нейрогенез)

---

## Два режима обучения в SYNAPSE

### 1. `train.py` - Классическое (для экспериментов)

```bash
python train.py --config fast   # 10K samples, 10 epochs
python train.py --config full   # 10M samples, 200 epochs
```

**Когда использовать:**
- Быстрые эксперименты
- Тестирование гипотез
- Отладка

### 2. `train_continuous.py` - Непрерывное (как мозг)

```bash
# Запустить и забыть (до Ctrl+C)
python train_continuous.py

# Остановить через 24 часа
python train_continuous.py --hours 24

# Остановить через 1M шагов
python train_continuous.py --steps 1000000

# Продолжить с чекпоинта
python train_continuous.py --resume continuous_1234567890.pt
```

**Когда использовать:**
- Основное обучение
- Долгосрочные эксперименты
- Реальная симуляция мозга

---

## Что происходит во время непрерывного обучения?

### Каждые несколько секунд:

```
Step    1234 | Φ=0.452 | Agency=0.723 | Time: 2m 34s
```

### Каждые 5 минут (автосохранение):

```
[Checkpoint] Saved to checkpoints/continuous_1234567890.pt (auto)
```

### При нейрогенезе (рост мозга):

```
[Neurogenesis] Growing self_model: High activation (0.87 > 0.85)
```

### При Ctrl+C:

```
Saving final checkpoint...

TRAINING SUMMARY
Total steps: 15,234
Total samples: 487,488
Training time: 2h 34m 12s
Final checkpoint: checkpoints/continuous_1234567890.pt

To resume:
  python train_continuous.py --resume continuous_1234567890.pt
```

---

## Архитектура непрерывного обучения

```
┌─────────────────────────────────────────────────────────────┐
│                    CONTINUOUS LEARNING                       │
├─────────────────────────────────────────────────────────────┤
│                                                               │
│  Experience Stream ──▶ Replay Buffer ◀─── Past Experiences   │
│        │                    │                                 │
│        ▼                    ▼                                 │
│  ┌──────────┐         ┌───────────┐                          │
│  │  Model   │────────▶│ Neurogenesis │ (grow when needed)    │
│  │  Step    │         └───────────┘                          │
│  └──────────┘              │                                 │
│        │                   ▼                                 │
│        ▼              New Neurons                             │
│  ┌──────────┐                                                │
│  │ Metrics  │──▶ Φ, Agency, Integration, Confidence          │
│  └──────────┘                                                │
│        │                                                      │
│        ▼                                                      │
│  ┌──────────────┐                                            │
│  │ Checkpoint   │ (every 5 min + on shutdown)               │
│  └──────────────┘                                            │
│                                                               │
└─────────────────────────────────────────────────────────────┘
```

---

## Нейрогенез (Neurogenesis)

### Что это?

**Рост новых нейронов** когда текущих недостаточно.

### Триггеры:

| Триггер | Порог | Действие |
|---------|-------|----------|
| Высокая активация | > 85% | Добавить 10% нейронов |
| Рост ошибки | +10% | Добавить нейроны |
| Information bottleneck | Обнаружен | Расширить слой |

### Лимиты:

```bash
--max-neurons 100000  # Безопасный предел
```

---

## Complementary Learning Systems (CLS)

Две системы памяти:

| Система | Скорость | Долговечность | Аналог |
|---------|----------|---------------|--------|
| **Fast** | Мгновенная | Кратковременная | Гиппокамп |
| **Slow** | Медленная | Долговременная | Кора |

### Как это работает:

1. **Новый опыт** → Fast system (гиппокамп)
2. **Во время "сна"** → Перенос в Slow system (кора)
3. **Replay** → Закрепление связей

### В коде:

```python
# Replay buffer = "sleep" consolidation
replay_ratio=0.2  # 20% батча из прошлого опыта
```

---

## Контрольные точки (Checkpoints)

### Автосохранение:

```
checkpoints/
├── continuous_1706123456.pt  # Авто, 5 мин назад
├── continuous_1706123756.pt  # Авто, 10 мин назад
├── continuous_1706124056.pt  # Авто, 15 мин назад
├── continuous_1706124356.pt  # Авто, 20 мин назад
└── continuous_1706124656.pt  # Последний
```

### Что внутри:

```python
{
    'step_count': 15234,
    'total_samples_seen': 487488,
    'model_state_dict': {...},
    'internal_state': tensor([...]),
    'history_buffer': [...],
    'metrics_history': [...],
    'replay_buffer': [...],
    'growth_history': [...],
    'timestamp': '2026-02-19T14:30:56',
    'reason': 'auto'
}
```

---

## Примеры использования

### Сценарий 1: Ночная тренировка

```bash
# Вечером запустили
python train_continuous.py --hours 8

# Утром - готовая модель
# Авто-сохранение каждые 5 минут
# Потеря максимум 5 минут прогресса
```

### Сценарий 2: Долгосрочный эксперимент

```bash
# День 1
python train_continuous.py --steps 500000
# Ctrl+C через 4 часа
# Saved: continuous_xxx.pt

# День 2 - продолжаем
python train_continuous.py --resume continuous_xxx.pt --steps 500000

# День 3, 4, 5... - накапливаем опыт
```

### Сценарий 3: С нейрогенезом

```bash
# Запускаем с ростом мозга
python train_continuous.py --neurogenesis --max-neurons 500000

# Мозг будет расти по мере необходимости
# От начальных ~100K до 500K нейронов
```

---

## Метрики для мониторинга

### Основные (в реальном времени):

| Метрика | Здоровый диапазон | Значение |
|---------|-------------------|----------|
| **Φ (Phi)** | 0.4 - 0.8 | Интеграция сознания |
| **Agency** | 0.6 - 0.9 | Чувство контроля |
| **Integration** | 0.5 - 0.9 | Унификация опыта |
| **Confidence** | 0.5 - 0.9 | Мета-уверенность |

### Тревожные сигналы:

| Сигнал | Проблема | Решение |
|--------|----------|---------|
| Φ падает | Потеря интеграции | Проверить связи |
| Agency < 0.3 | Нет чувства контроля | Увеличить learning rate |
| Ошибка растёт | Катастрофическое забывание | Уменьшить lr, увеличить replay |

---

## Практические рекомендации

### Сколько обучать?

| Цель | Время | Шаги |
|------|-------|------|
| Быстрый тест | 30 мин | ~10K |
| Базовая модель | 8 часов | ~500K |
| Хорошая модель | 2-3 дня | ~3M |
| Исследование | Недели | Миллионы |

### Когда останавливаться?

**Не по эпохам!** Смотрите на метрики:

1. **Φ стабилизировался** (не растёт 1 час)
2. **Agency > 0.7** стабильно
3. **Self-prediction error** минимален
4. **Нейрогенез замедлился** (мозг "насытился")

### Как следить за прогрессом?

```bash
# Запустить dashboard
run_dashboard.bat

# Или смотреть логи
tail -f training.log
```

---

## Отличие от классического ML

| Классическое ML | SYNAPSE (Мозг) |
|-----------------|----------------|
| Эпохи | Непрерывный поток |
| Фиксированная архитектура | Нейрогенез |
| Все данные сразу | Поток опыта |
| Нет "сна" | Replay (consolidation) |
| Одна модель | Progressive networks |
| Эпизодическое обучение | Incremental learning |

---

**Версия:** 2.0.0 (Continuous Learning)
**Обновлено:** 2026-02-19
